{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5587880-33e4-4aeb-abd3-1ccaa1e6cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import wandb\n",
    "import itertools\n",
    "import numpy as np\n",
    "from time import gmtime, strftime\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('../training')\n",
    "from utils import utils, data_utils, audio_utils\n",
    "from hyface import HyFace\n",
    "from networks.bshall import AcousticModel\n",
    "from networks.discriminator import Discriminator\n",
    "from datasets.loader import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e40eeb3d-cc50-4c9c-99e1-153feb2f9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = '/home/jaejun/nansy/\n",
    "config_path = '../training/configs/base.json'\n",
    "with open(config_path, \"r\") as f:\n",
    "    data = f.read()\n",
    "config = json.loads(data)\n",
    "args = utils.HParams(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c5cd9-c863-4244-b089-b7bf3855f402",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1d5012ee-05bb-4c5a-b476-e2adfee6a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Dataset(args, meta_root='../training/filelists', mode='train', datasets=['vctk'], sample_rate=args.data.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "99e11197-638d-4232-9b21-a0c64789f506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87879,), torch.Size([274, 256]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, hubert = trainset[0]\n",
    "audio.shape, hubert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ffffb6ca-69b1-4564-a579-84a56c100c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=4, collate_fn=trainset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1be3fce6-e87c-4629-ac43-03b75d7b7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "32af35a9-ae82-4288-839a-a730c42dc981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 87680), torch.Size([4, 256, 274]), (4,))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['audio'].shape, data['hubert'].shape, data['frame_lengths'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "df163bfc-285c-4de1-8a25-d171cddb6933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([548, 370, 438, 500])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['frame_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94ab3f-6d05-4c47-86b6-b9debe951330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f286a1dc-4817-4d81-8fdd-b1038d1c3d94",
   "metadata": {},
   "source": [
    "## HyFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34926d4f-7edd-4f5b-8ffe-63472ff56b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:611: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hyface = HyFace(args)\n",
    "disc = Discriminator(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c57899b0-31e1-4317-961a-e81d6e3d27b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tudio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43maudio\u001b[49m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tudio\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m tudio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m87879\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio' is not defined"
     ]
    }
   ],
   "source": [
    "tudio = torch.tensor(audio).unsqueeze(0)\n",
    "print(tudio.shape)\n",
    "tudio = torch.randn([4, 87879])\n",
    "hubert = torch.randn([4, 256, 274])\n",
    "tudio.shape, hubert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66828060-c8ae-4c64-b4dd-8db01317c3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80, 549])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel = hyface.logmel(tudio)\n",
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ccd6a7d-1768-4982-a44b-1e1082f9c431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 192]), torch.Size([4, 128, 50]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timbre_global, timbre_bank = hyface.analyze_timbre(tudio)\n",
    "timbre_global.shape, timbre_bank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad153792-7d0a-4bf0-b271-ecee958652d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 192, 274])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = torch.cat([hubert, timbre_global[...,None].repeat(1,1,hubert.shape[-1])],dim=1)\n",
    "timbre_sampled = hyface.timbre.sample_timber(contents, timbre_global, timbre_bank)\n",
    "timbre_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6db0e0b2-9790-4624-9b42-30bfd4d2f814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80, 548])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth = hyface.synthesize(hubert, timbre_global, timbre_bank)\n",
    "synth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65731cce-dd86-4cb0-8efa-72587d347cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "820d394b-3b82-4e5a-b267-fee77b4a7333",
   "metadata": {},
   "source": [
    "## BShall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7abad2f5-67e7-41f1-90e3-19933ff450ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = '/home/jaejun/nansy/\n",
    "config_path = '../training/configs/bshall.json'\n",
    "with open(config_path, \"r\") as f:\n",
    "    data = f.read()\n",
    "config = json.loads(data)\n",
    "args = utils.HParams(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6a0645e-6ef7-4767-992e-78022a2f6712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 87879]), torch.Size([4, 256, 274]), torch.Size([4, 192, 274]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tudio = torch.randn([4, 87879])\n",
    "hubert = torch.randn([4, 256, 274])\n",
    "timbre = torch.randn([4, 192, 274])\n",
    "tudio.shape, hubert.shape, timbre_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df553991-13eb-49f4-99bb-2623ef2e487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acmodel = AcousticModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2fae1e2d-d2b5-4bcc-9fc9-d040fb700262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.modules.utils import consume_prefix_in_state_dict_if_present\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, prenet_indim: int, upsample: bool = True):\n",
    "        super().__init__()\n",
    "        self.prenet = PreNet(prenet_indim, 256, 256)\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv1d(256, 512, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.InstanceNorm1d(512),\n",
    "            nn.ConvTranspose1d(512, 512, 4, 2, 1) if upsample else nn.Identity(),\n",
    "            nn.Conv1d(512, 512, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.InstanceNorm1d(512),\n",
    "            nn.Conv1d(512, 512, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.InstanceNorm1d(512),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.prenet(x)\n",
    "        x = self.convs(x.transpose(1, 2))\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "class PreNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # nn.Conv1d(input_size, hidden_size, 1),\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            # nn.Conv1d(hidden_size, output_size, 1),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(1024, 1024, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(1024, 1024, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(1024, 1024, batch_first=True)\n",
    "        self.proj = nn.Linear(1024, 80, bias=False)\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mels: torch.Tensor) -> torch.Tensor:\n",
    "        x  = torch.cat((x, mels), dim=-1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        res = x\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = res + x\n",
    "        res = x\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = res + x\n",
    "        return self.proj(x)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, xs: torch.Tensor) -> torch.Tensor:\n",
    "        m = torch.zeros(xs.size(0), 80, device=xs.device)\n",
    "        h1 = torch.zeros(1, xs.size(0), 1024, device=xs.device)\n",
    "        c1 = torch.zeros(1, xs.size(0), 1024, device=xs.device)\n",
    "        h2 = torch.zeros(1, xs.size(0), 1024, device=xs.device)\n",
    "        c2 = torch.zeros(1, xs.size(0), 1024, device=xs.device)\n",
    "        h3 = torch.zeros(1, xs.size(0), 1024, device=xs.device)\n",
    "        c3 = torch.zeros(1, xs.size(0), 1024, device=xs.device)\n",
    "\n",
    "        mel = []\n",
    "        for x in torch.unbind(xs, dim=1):\n",
    "            x = torch.cat((x, m), dim=1).unsqueeze(1)\n",
    "            x1, (h1, c1) = self.lstm1(x, (h1, c1))\n",
    "            x2, (h2, c2) = self.lstm2(x1, (h2, c2))\n",
    "            x = x1 + x2\n",
    "            x3, (h3, c3) = self.lstm3(x, (h3, c3))\n",
    "            x = x + x3\n",
    "            m = self.proj(x).squeeze(1)\n",
    "            mel.append(m)\n",
    "        return torch.stack(mel, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4386e921-0511-413f-8b8e-f2858c1a0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(256, True)\n",
    "enc2 = Encoder(192, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "097ebc8d-c024-4ce9-84b4-b44496baa104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 548, 512]), torch.Size([4, 548, 512]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = enc(hubert.transpose(1,2))\n",
    "y = enc2(timbre.transpose(1,2))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "80cc7883-c08f-40e0-acba-fdd8e095b7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 548, 1024])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catt = torch.cat((x, y), dim=-1)\n",
    "catt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d7fe6664-ada0-4b8a-bbc0-26cacb3052f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 548, 128])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(args)\n",
    "z = decoder(x, y)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544427a-7242-4204-82b1-2b8eac2d1c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0427ef7a-7273-4c02-98fc-618a81bc2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn([4, 80, 100])\n",
    "b = torch.randn([4, 80, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "945c40d6-c6ab-4095-b890-cfd52e04719a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 80, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7204, -0.7130, -3.0918,  ...,  2.1229, -1.9494, -0.7566],\n",
       "        [-0.0798,  1.2675, -0.9218,  ...,  0.5414, -0.3596,  0.1896],\n",
       "        [-0.0225,  0.3200,  0.6995,  ...,  0.0965,  0.7835,  0.3701],\n",
       "        ...,\n",
       "        [ 1.6791, -1.1309, -0.1175,  ..., -1.7433, -1.1525, -0.1156],\n",
       "        [-1.7790,  0.8525, -0.5253,  ...,  0.7850, -0.0322,  0.5535],\n",
       "        [-1.2482, -2.1930,  2.0543,  ..., -0.7643,  1.7816,  1.3197]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a-b\n",
    "print(c.shape)\n",
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "73720afd-fecc-480e-ac04-76241fed35f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1425)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "41624224-6acb-45f3-b400-200a64194791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 80, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.7204, 0.7130, 3.0918,  ..., 2.1229, 1.9494, 0.7566],\n",
       "        [0.0798, 1.2675, 0.9218,  ..., 0.5414, 0.3596, 0.1896],\n",
       "        [0.0225, 0.3200, 0.6995,  ..., 0.0965, 0.7835, 0.3701],\n",
       "        ...,\n",
       "        [1.6791, 1.1309, 0.1175,  ..., 1.7433, 1.1525, 0.1156],\n",
       "        [1.7790, 0.8525, 0.5253,  ..., 0.7850, 0.0322, 0.5535],\n",
       "        [1.2482, 2.1930, 2.0543,  ..., 0.7643, 1.7816, 1.3197]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.l1_loss(a, b, reduction=\"none\")\n",
    "print(loss.shape)\n",
    "loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "af90305e-a3e1-4dfb-a0b9-062499d4e6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16.8403, 24.3411, 20.6968, 18.5207], dtype=torch.float64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loss, dim=(1,2)) / data['frame_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "376ff8f1-e366-469f-a7bd-b3a25d6a09e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9228.4756, 9006.2051, 9065.1963, 9260.3457])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loss, dim=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9c110950-f333-4b32-96e8-38e2d6842517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([548, 370, 438, 500])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['frame_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0102c4-7cf7-43b2-a080-c32316b86f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
