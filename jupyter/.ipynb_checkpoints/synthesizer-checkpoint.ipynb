{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce020bb1-104b-4f3c-b941-0e8011858fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('../training')\n",
    "from networks.wavenet import WaveNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f9966-0769-4db9-97cb-ba5eee977177",
   "metadata": {},
   "source": [
    "## Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d0ffc60-8ddf-4d00-93fd-281f501ab5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 200]) torch.Size([4, 192, 200]) torch.Size([4, 256, 100])\n",
      "torch.Size([4, 191, 200]) torch.Size([4, 200]) torch.Size([4, 200]) torch.Size([4, 200])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "time_frame = 200\n",
    "lin_out = torch.randn([batch_size, 128, time_frame])\n",
    "tv_timbre = torch.randn([batch_size, 192, time_frame])\n",
    "hubert_temp = torch.randn([batch_size, 256, time_frame//2])\n",
    "print(lin_out.shape, tv_timbre.shape, hubert_temp.shape)\n",
    "\n",
    "cqt = torch.randn([batch_size, 191, time_frame])\n",
    "pitch = torch.randn([batch_size, time_frame])\n",
    "p_amp = torch.randn([batch_size, time_frame])\n",
    "ap_amp = torch.randn([batch_size, time_frame])\n",
    "print(cqt.shape, pitch.shape, p_amp.shape, ap_amp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0957f216-5a61-453f-851f-cd93aa3bd6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hubert_dir = \"/disk2/vctk/modified/hubert_soft/p225/p225_001.emb\"\n",
    "hubert_emb = torch.load(hubert_dir)\n",
    "hubert_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be73fd-22e9-4a1f-8942-2d9903345d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d569e-d4d4-479e-8b50-195d7ff549ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b235542e-fcaa-4900-a0e5-8640c78f3fb7",
   "metadata": {},
   "source": [
    "## Frame-level Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0123477-3932-446b-9399-b8b67a784b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGLU(nn.Module):\n",
    "    \"\"\"Dropout - Conv1d - GLU and conditional layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 channels: int,\n",
    "                 kernels: int,\n",
    "                 dilations: int,\n",
    "                 dropout: float,\n",
    "                 cond: Optional[int] = None):\n",
    "        \"\"\"Initializer.\n",
    "        Args:\n",
    "            channels: size of the input channels.\n",
    "            kernels: size of the convolutional kernels.\n",
    "            dilations: dilation rate of the convolution.\n",
    "            dropout: dropout rate.\n",
    "            cond: size of the condition channels, if provided.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(channels, channels * 2, kernels, dilation=dilations,\n",
    "                      padding=(kernels - 1) * dilations // 2),\n",
    "            nn.GLU(dim=1))\n",
    "        \n",
    "        self.cond = cond\n",
    "        if cond is not None:\n",
    "            self.cond = nn.Conv1d(cond, channels * 2, 1)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, cond: Optional[torch.Tensor] = None):\n",
    "        \"\"\"Transform the inputs with given conditions.\n",
    "        Args:\n",
    "            inputs: [torch.float32; [B, channels, T]], input channels.\n",
    "            cond: [torch.float32; [B, cond, T]], if provided.\n",
    "        Returns:\n",
    "            [torch.float32; [B, channels, T]], transformed.\n",
    "        \"\"\"\n",
    "        # [B, channels, T]\n",
    "        x = inputs + self.conv(inputs)\n",
    "        if cond is not None:\n",
    "            assert self.cond is not None, 'condition module does not exists'\n",
    "            # [B, channels, T]\n",
    "            x = F.instance_norm(x, use_input_stats=True)\n",
    "            # [B, channels, T]\n",
    "            weight, bias = self.cond(cond).chunk(2, dim=1)\n",
    "            # [B, channels, T]\n",
    "            x = x * weight + bias\n",
    "        return x\n",
    "\n",
    "\n",
    "class CondSequential(nn.Module):\n",
    "    \"\"\"Sequential pass with conditional inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, modules: List[nn.Module]):\n",
    "        \"\"\"Initializer.\n",
    "        Args:\n",
    "            modules: list of torch modules.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lists = nn.ModuleList(modules)\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"Pass the inputs to modules.\n",
    "        Args:\n",
    "            inputs: arbitary input tensors.\n",
    "            args, kwargs: positional, keyword arguments.\n",
    "        Returns:\n",
    "            output tensor.\n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "        for module in self.lists:\n",
    "            x = module.forward(x, *args, **kwargs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FrameLevelSynthesizer(nn.Module):\n",
    "    \"\"\"Frame-level synthesizer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 channels: int,\n",
    "                 embed: int,\n",
    "                 kernels: int,\n",
    "                 dilations: List[int],\n",
    "                 blocks: int,\n",
    "                 leak: float,\n",
    "                 dropout: float):\n",
    "        \"\"\"Initializer.\n",
    "        Args:\n",
    "            channels: size of the input channels.\n",
    "            embed: size of the time-varying timber embeddings.\n",
    "            kernels: size of the convolutional kernels.\n",
    "            dilations: dilation rates.\n",
    "            blocks: the number of the 1x1 ConvGLU blocks after dilated ConvGLU.\n",
    "            leak: negative slope of the leaky relu.\n",
    "            dropout: dropout rates.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # channels=1024\n",
    "        # unknown `leak`, `dropout`\n",
    "        self.preconv = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels, 1),\n",
    "            nn.LeakyReLU(leak),\n",
    "            nn.Dropout(dropout))\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.InstanceNorm1d(channels),\n",
    "            nn.ConvTranspose1d(channels, channels, 4, 2, 1),\n",
    "            nn.Conv1d(channels, channels, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.InstanceNorm1d(channels),\n",
    "            nn.Conv1d(channels, channels, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.InstanceNorm1d(channels),\n",
    "            )\n",
    "        # kernels=3, dilations=[1, 3, 9, 27, 1, 3, 9, 27], blocks=2\n",
    "        self.convglu = CondSequential(\n",
    "            [\n",
    "                ConvGLU(channels, kernels, dilation, dropout, cond=embed)\n",
    "                for dilation in dilations]\n",
    "            + [\n",
    "                ConvGLU(channels, 1, 1, dropout, cond=embed)\n",
    "                for _ in range(blocks)])\n",
    "\n",
    "        self.proj = nn.Conv1d(channels, channels, 1)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, timber: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Synthesize in frame-level.\n",
    "        Args:\n",
    "            inputs: [torch.float32; [B, channels, T]], input features.\n",
    "            timber: [torch.float32; [B, embed, T]], time-varying timber embeddings.\n",
    "        Returns;\n",
    "            [torch.float32; [B, channels, T]], outputs.\n",
    "        \"\"\"\n",
    "        # [B, channels, T]\n",
    "        x = self.preconv(inputs)\n",
    "        # [B, channels, 2T]\n",
    "        x = self.upsample(x)\n",
    "        # [B, channels, 2T] \n",
    "        x = self.convglu(x, timber)\n",
    "        # [B, channels, 2T]\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "202949e0-a903-421e-be6f-31cd2dd8794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsynthesizer = FrameLevelSynthesizer(256, 192, 3, [1, 3, 9, 27, 1, 3, 9, 27], 2, 0.01, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f22e799-0789-46ea-a272-6af65a781fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 200])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcondition = fsynthesizer(hubert_temp, tv_timbre)\n",
    "fcondition.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571bfd47-cd0b-40bd-ae31-baa874735af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b82e6157-aef8-4447-a4ba-ae66905f4b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 200]), torch.Size([4, 64000]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampler = nn.Upsample(scale_factor=320, mode='linear')\n",
    "new_pitch = upsampler(pitch[:,None]).squeeze(dim=1)\n",
    "pitch[:,None].shape, new_pitch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056b004-eb3f-4d7b-8635-50d5623e2fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b28b6a-9220-42fb-b655-d0e980e8fb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d55966-eb36-4aa4-8ca3-ac09f5e3c136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65253daa-a769-4e62-b8e3-3157d77647de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4edc4dd-98f9-4530-b513-7c22c35ecfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342b55f-8982-4aa4-8476-a571b3d4559b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb6c12-feb9-4583-a337-a57a6a552bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
